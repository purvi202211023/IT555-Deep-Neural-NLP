{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvi202211023/IT555-Deep-Neural-NLP/blob/Train-Word2Vec-on-peS2o-Dataset-(AllenNLP)-_Assignment1/Deep_Neural_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import inflect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from num2words import num2words  # For numerical-to-text conversion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT5YvJirBr49",
        "outputId": "dd64b725-f498-4e3b-d22a-aa8cdd4c8d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/125.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=8d7065fdd41f3d388c6cc0710431ffcc5aa3216be8d0ba69396064693f2624a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrc-ekuiBudf",
        "outputId": "05e6b9e5-ea68-4524-a7db-ed3db6e0b86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTVXbrn5BN1b",
        "outputId": "723b84a9-5b02-47a9-d21e-c1bbfb91815c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLCdcQLd9kat"
      },
      "outputs": [],
      "source": [
        "# Load the training data\n",
        "df_train_n = pd.read_pickle(\"/content/drive/MyDrive/NLP/train_data_n.pkl\")\n",
        "\n",
        "# Load the test data\n",
        "df_test_n = pd.read_pickle(\"/content/drive/MyDrive/NLP/test_data_n.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_df = pd.concat([df_train_n, df_test_n], ignore_index=True)\n",
        "# combined_df"
      ],
      "metadata": {
        "id": "DgdyvL5KE-tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an inflect engine for number-to-text conversion\n",
        "p = inflect.engine()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove bullets\n",
        "    text = re.sub(r'\\u2022', '', text)\n",
        "\n",
        "    # Handle apostrophe\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "\n",
        "    # Handle hyphens\n",
        "    text = text.replace(\"-\", \" \")\n",
        "\n",
        "    # Handle enumerations\n",
        "    text = re.sub(r'\\(i\\)|\\(ii\\)|\\(iii\\)|\\(iv\\)|\\(v\\)|\\(vi\\)|\\(vii\\)|\\(viii\\)|\\(ix\\)|\\(x\\)', '', text)\n",
        "\n",
        "    # Convert numbers to text\n",
        "    text = ' '.join([p.number_to_words(word) if bool(re.search(r'\\d', word)) else word for word in text.split()])\n",
        "\n",
        "    # Remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Tokenize the text in df_train into sentences\n",
        "sentences_train = [sent_tokenize(text) for text in df_train_n['text']]\n",
        "sentences_train = [sentence for sublist in sentences_train for sentence in sublist]  # Flatten the list\n",
        "\n",
        "# Apply the pre-processing function to each sentence\n",
        "processed_sentences = [preprocess_text(sentence) for sentence in sentences_train]\n",
        "\n",
        "processed_sentences[:5]  # Display first 5 processed sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EyLAiVdCzMt",
        "outputId": "e5bb3b09-61f3-4c5e-8795-ee74c8ccfbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['automatic question generation five one open domain indonesian questions using syntactical template based features academic textbooks measuring education quality school conducted delivering examination students',\n",
              " 'composing questions examination process measure students achievement school teaching learning process difficult time consuming',\n",
              " 'solve problem research proposes automatic question generation aqg method generate open domain indonesian question using syntactical approach',\n",
              " 'open domain questions questions covering many domains knowledge',\n",
              " 'challenge generating questions identify types declarative sentences potential transformed questions develop method generating question automatically']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_numbers(processed_sentences):\n",
        "    sentences_with_numbers = [sentence for sentence in processed_sentences if bool(re.search(r'\\d', sentence))]\n",
        "    return sentences_with_numbers\n",
        "\n",
        "# Using the function\n",
        "sentences_with_numbers = check_for_numbers(processed_sentences)\n",
        "\n",
        "if sentences_with_numbers:\n",
        "    print(f\"Found {len(sentences_with_numbers)} sentences with numbers:\")\n",
        "    for sent in sentences_with_numbers:\n",
        "        print(sent)\n",
        "else:\n",
        "    print(\"No numbers found in the processed sentences!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRh7Yi3J45zS",
        "outputId": "9a9d2a51-b260-4fd7-fb03-58f0c92bc1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No numbers found in the processed sentences!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ-06b0WD_Pm",
        "outputId": "d190fe81-579b-432a-c586-a4617e65a860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26755"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to apply all preprocessing steps\n",
        "# def preprocess_text(text):\n",
        "#     # Convert text to lowercase\n",
        "#     text = text.lower()\n",
        "\n",
        "#     # Remove URLs\n",
        "#     text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "#     # Remove bullets and enumerations\n",
        "#     text = re.sub(r'•|\\d+\\.', '', text)\n",
        "\n",
        "#     # Remove apostrophes\n",
        "#     text = re.sub(r\"([’'‘’])\", '', text)\n",
        "\n",
        "#     # Remove hyphens\n",
        "#     text = re.sub(r\"-\", ' ', text)\n",
        "\n",
        "#     # Convert numerical digits to text\n",
        "#     text = ' '.join([num2words(word) if word.isdigit() else word for word in text.split()])\n",
        "\n",
        "#     # Tokenize the text\n",
        "#     tokens = word_tokenize(text)\n",
        "\n",
        "#     # Remove stopwords\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "#     # Remove punctuation\n",
        "#     tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "#     # Join tokens back into a single string\n",
        "#     text = ' '.join(tokens)\n",
        "\n",
        "#     return text\n",
        "\n",
        "# # Apply the preprocessing function to the \"text\" column\n",
        "# combined_df['text'] = combined_df['text'].apply(preprocess_text)\n",
        "\n",
        "# # Print the preprocessed DataFrame\n",
        "# print(combined_df)"
      ],
      "metadata": {
        "id": "8ckDc0VGG9iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the training data and transform it to create the context matrix\n",
        "context_matrix_train = tfidf_vectorizer.fit_transform(processed_sentences)\n"
      ],
      "metadata": {
        "id": "Xix9HYAwI1Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "Do9PSYczKuTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the TF-IDF context matrix to a DataFrame for easier viewing\n",
        "context_matrix_train_df = pd.DataFrame(context_matrix_train.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the DataFrame\n",
        "print(context_matrix_train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "126We2PUKj6y",
        "outputId": "43801bd2-fdc8-416b-c7de-13f85555de08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ____________________________________________   aa  aaa  aaai  aaate  \\\n",
            "0                                               0.0  0.0  0.0   0.0    0.0   \n",
            "1                                               0.0  0.0  0.0   0.0    0.0   \n",
            "2                                               0.0  0.0  0.0   0.0    0.0   \n",
            "3                                               0.0  0.0  0.0   0.0    0.0   \n",
            "4                                               0.0  0.0  0.0   0.0    0.0   \n",
            "...                                             ...  ...  ...   ...    ...   \n",
            "26750                                           0.0  0.0  0.0   0.0    0.0   \n",
            "26751                                           0.0  0.0  0.0   0.0    0.0   \n",
            "26752                                           0.0  0.0  0.0   0.0    0.0   \n",
            "26753                                           0.0  0.0  0.0   0.0    0.0   \n",
            "26754                                           0.0  0.0  0.0   0.0    0.0   \n",
            "\n",
            "       aab  aachen  aacr  aals  aalto  ...  ﬁnally  ﬁnd  ﬁndings  ﬁnite  \\\n",
            "0      0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "1      0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "2      0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "3      0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "4      0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "...    ...     ...   ...   ...    ...  ...     ...  ...      ...    ...   \n",
            "26750  0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "26751  0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "26752  0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "26753  0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "26754  0.0     0.0   0.0   0.0    0.0  ...     0.0  0.0      0.0    0.0   \n",
            "\n",
            "       ﬁniteclass  ﬁre  ﬁrst  ﬁve  ﬂight  ﬂows  \n",
            "0             0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "1             0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "2             0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "3             0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "4             0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "...           ...  ...   ...  ...    ...   ...  \n",
            "26750         0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "26751         0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "26752         0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "26753         0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "26754         0.0  0.0   0.0  0.0    0.0   0.0  \n",
            "\n",
            "[26755 rows x 34176 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multiply matrix with hot vector"
      ],
      "metadata": {
        "id": "msaTL6qLgwCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# # Tokenize the sentences in your preprocessed training data\n",
        "tokenized_sentences = [sentence.split() for sentence in processed_sentences]\n",
        "\n",
        "# # Set Word2Vec hyperparameters\n",
        "# vector_size = 100  # Adjust the vector size as needed\n",
        "# window = 5  # Adjust the context window size as needed\n",
        "# min_count = 1  # Minimum word count, adjust as needed\n",
        "# sg = 0  # Use CBOW (Continuous Bag of Words) model, set to 1 for Skip-gram\n",
        "\n",
        "# # Initialize and train the Word2Vec model\n",
        "# word2vec_model = Word2Vec(tokenized_sentences, vector_size=vector_size, window=window, min_count=min_count, sg=sg)\n"
      ],
      "metadata": {
        "id": "nVZYUUDLX-PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=100)"
      ],
      "metadata": {
        "id": "5EuCOTdVBjSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb99ccb-d117-48a7-c9c3-f39862f7b9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42526582, 43777600)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_weighted_word2vec_optimized(tfidf_matrix, tfidf_feature_names, w2v_model):\n",
        "    # Precompute word embeddings for all the vocabulary words\n",
        "    word_embeddings = {word: w2v_model.wv[word] for word in tfidf_feature_names if word in w2v_model.wv}\n",
        "\n",
        "    # Convert word embeddings to a matrix form\n",
        "    embedding_matrix = np.zeros((len(tfidf_feature_names), w2v_model.vector_size))\n",
        "    for i, word in enumerate(tfidf_feature_names):\n",
        "        if word in word_embeddings:\n",
        "            embedding_matrix[i] = word_embeddings[word]\n",
        "\n",
        "    # Use matrix multiplication to get the document embeddings\n",
        "    doc_embeddings = tfidf_matrix.dot(embedding_matrix)\n",
        "\n",
        "    return doc_embeddings"
      ],
      "metadata": {
        "id": "yFvH6_Tl5rmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embeddings = tfidf_weighted_word2vec_optimized(context_matrix_train, tfidf_feature_names, model)\n",
        "print(doc_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvBRN8mH5vX2",
        "outputId": "8cca5fa0-b8ca-4940-e126-98293e257d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.57849914 -4.05980114  0.41776092 ...  0.91950865  1.60054729\n",
            "   0.9299773 ]\n",
            " [ 2.57997322 -0.20409037  2.20582195 ...  1.25120234 -1.71415103\n",
            "   3.10640914]\n",
            " [ 2.36373818  0.04158967  1.52687676 ...  3.74448126 -2.2098694\n",
            "   1.19911955]\n",
            " ...\n",
            " [ 1.99483204 -2.19037168 -0.51199073 ...  1.54914768 -0.4968133\n",
            "   1.6805061 ]\n",
            " [ 1.33960938 -0.8910132   0.76457774 ...  3.12616178 -0.3089393\n",
            "   0.39292261]\n",
            " [ 0.92538037  3.55945726 -2.32647297 ...  0.54026678  0.36990689\n",
            "   3.76433626]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create TF-IDF-weighted Word2Vec representations\n",
        "# def tfidf_weighted_word2vec(tfidf_matrix, tfidf_feature_names, w2v_model):\n",
        "#     doc_embeddings = []\n",
        "\n",
        "#     for doc_tfidf in tfidf_matrix:\n",
        "#         doc_embedding = np.zeros(w2v_model.vector_size)\n",
        "#         for word, weight in zip(tfidf_feature_names, doc_tfidf.toarray()[0]):\n",
        "#             if word in w2v_model.wv:\n",
        "#                 doc_embedding += weight * w2v_model.wv[word]\n",
        "#         doc_embeddings.append(doc_embedding)\n",
        "\n",
        "#     return np.array(doc_embeddings)\n",
        "\n",
        "# doc_embeddings = tfidf_weighted_word2vec(context_matrix_train, tfidf_feature_names, model)\n",
        "# print(doc_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "uGqSghkML92Y",
        "outputId": "775f1dcb-6175-41d0-f925-42b6b573b6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7bd255b973bf>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdoc_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_weighted_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_matrix_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-7bd255b973bf>\u001b[0m in \u001b[0;36mtfidf_weighted_word2vec\u001b[0;34m(tfidf_matrix, tfidf_feature_names, w2v_model)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdoc_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \"\"\"\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvi202211023/IT555-Deep-Neural-NLP/blob/Train-Word2Vec-on-peS2o-Dataset-(AllenNLP)-_Assignment1/Deep_Neural_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "**Team NeuronicNav**\n",
        "\n",
        "Assignment 1: Train Word2Vec on peS2o Dataset (AllenNLP)\n",
        "\n",
        "Id No.: 202211023, 202211004, 202221004\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oVFJKBXSIyLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import inflect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from num2words import num2words  # For numerical-to-text conversion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT5YvJirBr49",
        "outputId": "21a6da3a-d27b-44c9-d169-a1c483a6c958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrc-ekuiBudf",
        "outputId": "421e377b-35b7-4e70-d2d1-76d5c952c88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTVXbrn5BN1b",
        "outputId": "516a468b-dc04-43f0-97ac-81ffce3c97e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLCdcQLd9kat"
      },
      "outputs": [],
      "source": [
        "# # Load the training data\n",
        "# df_train_n = pd.read_pickle(\"/content/drive/MyDrive/NLP/train_data_n.pkl\")\n",
        "\n",
        "# # Load the test data\n",
        "# df_test_n = pd.read_pickle(\"/content/drive/MyDrive/NLP/test_data_n.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Research papers are collected only from Computer science domain\n",
        "\n",
        "\n",
        "  I have collected 4000 documents from a dataset and then store in one pickle file that I load here for the further use. I'll use bunch of keyword for filters CS domain's paper\n",
        "\n",
        "  \n",
        "Q2: 2. A random training data of 3000 documents are used, while a test dataset of 1000 documents is to be used."
      ],
      "metadata": {
        "id": "h-76_GphEmg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle(\"/content/drive/MyDrive/Datasets/df_new.pkl\")\n",
        "df_train = df[:3000]\n",
        "df_test = df[3000:4000]"
      ],
      "metadata": {
        "id": "DgdyvL5KE-tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "rwx0Yqt4KbxC",
        "outputId": "a24dd614-cbd2-4234-9f59-373bae743ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             id      source version                     added  \\\n",
              "0      79374620  s2ag/train      v1  2019-03-16T13:05:58.356Z   \n",
              "1      53596570  s2ag/train      v1  2018-11-07T20:43:16.101Z   \n",
              "2      60680010  s2ag/train      v1  2019-02-13T14:02:18.109Z   \n",
              "3     254100620  s2ag/train      v1  2022-12-01T14:30:17.430Z   \n",
              "4      17912070  s2ag/train      v1  2014-10-01T00:00:00.000Z   \n",
              "...         ...         ...     ...                       ...   \n",
              "3995   29202040  s2ag/train      v1  2017-09-06T09:13:11.852Z   \n",
              "3996   64350490  s2ag/train      v1  2019-02-16T14:30:11.842Z   \n",
              "3997   62537940  s2ag/train      v1  2019-02-14T14:17:03.140Z   \n",
              "3998    7418240  s2ag/train      v1  2017-02-10T07:10:48.762Z   \n",
              "3999  229412040  s2ag/train      v1  2020-12-10T09:08:07.984Z   \n",
              "\n",
              "                       created  \\\n",
              "0     2006-01-01T00:00:00.000Z   \n",
              "1     2003-09-24T00:00:00.000Z   \n",
              "2     2004-01-01T00:00:00.000Z   \n",
              "3     2022-10-10T00:00:00.000Z   \n",
              "4     2001-01-01T00:00:00.000Z   \n",
              "...                        ...   \n",
              "3995  2016-01-01T00:00:00.000Z   \n",
              "3996  2014-03-24T00:00:00.000Z   \n",
              "3997  1997-07-14T00:00:00.000Z   \n",
              "3998  2004-11-02T00:00:00.000Z   \n",
              "3999  2020-12-02T00:00:00.000Z   \n",
              "\n",
              "                                                   text  \n",
              "0     In-time rating of continuously cast semis by m...  \n",
              "1     Development of an Inspection System for Cracks...  \n",
              "2     Assessment of Wave-Induced Liquefaction in a P...  \n",
              "3     Prediction of Cascading Failures and Simultane...  \n",
              "4     2 3 Ju l 2 00 1 A Logical Framework for Conver...  \n",
              "...                                                 ...  \n",
              "3995  Computational Social Science: Discovery and Pr...  \n",
              "3996  Toward Automated Interpretation of Integrated ...  \n",
              "3997  Application of virtual reality technology to c...  \n",
              "3998  Three level method using machine learning and ...  \n",
              "3999  A Survey on Ship Intelligent Cabin\\n\\nAt prese...  \n",
              "\n",
              "[4000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71773799-7379-4501-b900-49f9361ee527\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>version</th>\n",
              "      <th>added</th>\n",
              "      <th>created</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>79374620</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2019-03-16T13:05:58.356Z</td>\n",
              "      <td>2006-01-01T00:00:00.000Z</td>\n",
              "      <td>In-time rating of continuously cast semis by m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53596570</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2018-11-07T20:43:16.101Z</td>\n",
              "      <td>2003-09-24T00:00:00.000Z</td>\n",
              "      <td>Development of an Inspection System for Cracks...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60680010</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2019-02-13T14:02:18.109Z</td>\n",
              "      <td>2004-01-01T00:00:00.000Z</td>\n",
              "      <td>Assessment of Wave-Induced Liquefaction in a P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>254100620</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2022-12-01T14:30:17.430Z</td>\n",
              "      <td>2022-10-10T00:00:00.000Z</td>\n",
              "      <td>Prediction of Cascading Failures and Simultane...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17912070</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2014-10-01T00:00:00.000Z</td>\n",
              "      <td>2001-01-01T00:00:00.000Z</td>\n",
              "      <td>2 3 Ju l 2 00 1 A Logical Framework for Conver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>29202040</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2017-09-06T09:13:11.852Z</td>\n",
              "      <td>2016-01-01T00:00:00.000Z</td>\n",
              "      <td>Computational Social Science: Discovery and Pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>64350490</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2019-02-16T14:30:11.842Z</td>\n",
              "      <td>2014-03-24T00:00:00.000Z</td>\n",
              "      <td>Toward Automated Interpretation of Integrated ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>62537940</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2019-02-14T14:17:03.140Z</td>\n",
              "      <td>1997-07-14T00:00:00.000Z</td>\n",
              "      <td>Application of virtual reality technology to c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>7418240</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2017-02-10T07:10:48.762Z</td>\n",
              "      <td>2004-11-02T00:00:00.000Z</td>\n",
              "      <td>Three level method using machine learning and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>229412040</td>\n",
              "      <td>s2ag/train</td>\n",
              "      <td>v1</td>\n",
              "      <td>2020-12-10T09:08:07.984Z</td>\n",
              "      <td>2020-12-02T00:00:00.000Z</td>\n",
              "      <td>A Survey on Ship Intelligent Cabin\\n\\nAt prese...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71773799-7379-4501-b900-49f9361ee527')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71773799-7379-4501-b900-49f9361ee527 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71773799-7379-4501-b900-49f9361ee527');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-431702e2-6922-494b-be9a-d88e2c6d627c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-431702e2-6922-494b-be9a-d88e2c6d627c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-431702e2-6922-494b-be9a-d88e2c6d627c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: The linguistic unit (row vector) in your input context matrix should be sentences (and not words). That is, you are to embed sentences instead of words.\n",
        "\n",
        "Q4: The following pre-processing cleanup needs to be done on the input context matrix: (i) stopwords, (ii) urls, (iii) bullets, (iv) apostrophe, (v) hyphens, (vi) enumerations (eg. like how this list is being enumerated), (vii), numerical-to-text conversion, (viii) punctuations."
      ],
      "metadata": {
        "id": "4GGLOIZcHGX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an inflect engine for number-to-text conversion\n",
        "p = inflect.engine()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove bullets\n",
        "    text = re.sub(r'\\u2022', '', text)\n",
        "\n",
        "    # Handle apostrophe\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "\n",
        "    # Handle hyphens\n",
        "    text = text.replace(\"-\", \" \")\n",
        "    text = text.replace(\"_\", \" \")\n",
        "\n",
        "    # Handle enumerations\n",
        "    text = re.sub(r'\\(i\\)|\\(ii\\)|\\(iii\\)|\\(iv\\)|\\(v\\)|\\(vi\\)|\\(vii\\)|\\(viii\\)|\\(ix\\)|\\(x\\)', '', text)\n",
        "\n",
        "    # Convert numbers to text\n",
        "    text = ' '.join([p.number_to_words(word) if bool(re.search(r'\\d', word)) else word for word in text.split()])\n",
        "\n",
        "    # Remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Tokenize the text in df_train into sentences\n",
        "sentences_train = [sent_tokenize(text) for text in df_train['text']]\n",
        "sentences_train = [sentence for sublist in sentences_train for sentence in sublist]  # Flatten the list\n",
        "\n",
        "# Apply the pre-processing function to each sentence\n",
        "processed_sentences = [preprocess_text(sentence) for sentence in sentences_train]\n",
        "\n",
        "processed_sentences[:5]  # Display first 5 processed sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EyLAiVdCzMt",
        "outputId": "e3cdf2bb-adc3-4bf4-a4ce-68c3b05d7a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['time rating continuously cast semis means hybrid quality models efficient reliable time determination surface internal quality continuous cast products deliver considerable contribution cost reduction efficiency improvement steel plants',\n",
              " 'currently economic losses environmental impacts caused downgrading slabs billets additionally due conditioning increased material energy consumption occur',\n",
              " 'nowadays development quality prediction models supported availability automatic inspection systems eg',\n",
              " 'surface inspection hot cold rolled strips bars',\n",
              " 'developing validating quality model concerning defects occur downstream process steps casting information inspection system reliable human decisions model adjusted accuracy']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(processed_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsKDgABPClMy",
        "outputId": "a6860d36-c951-4b4f-f9a0-b424fc9139da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text in df_train into sentences\n",
        "sentences_test = [sent_tokenize(text) for text in df_test['text']]\n",
        "sentences_test = [sentence for sublist in sentences_test for sentence in sublist]  # Flatten the list\n",
        "\n",
        "# Apply the pre-processing function to each sentence\n",
        "processed_sentences_test = [preprocess_text(sentence) for sentence in sentences_test]\n",
        "\n",
        "processed_sentences_test[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H1yIZArKG4n",
        "outputId": "9c021758-afa6-421c-c4eb-8050ab658db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['simulated annealing metaheuristic solve optimal power flow optimal power flow problem widely studied order improve power systems operation planning',\n",
              " 'real power systems problem formulated non linear large combinatorial problem',\n",
              " 'first approaches used solve problem based mathematical methods required huge computational efforts',\n",
              " 'lately artificial intelligence techniques metaheuristics based biological processes adopted',\n",
              " 'metaheuristics require lower computational resources clear advantage addressing problem large power systems']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(processed_sentences_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ngnofHKenr",
        "outputId": "1c35e9d0-b347-41de-8220-e4499876eaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF weighted word2vec"
      ],
      "metadata": {
        "id": "_IvLJwymHwQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "from dataclasses import dataclass\n",
        "from time import monotonic\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.spatial.distance import cosine\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import WikiText103\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Cj2yL48lVEeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "def get_data(train_sentences, valid_sentences):\n",
        "    train_dataset = MyDataset(train_sentences)\n",
        "    valid_dataset = MyDataset(valid_sentences)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader\n",
        "\n",
        "# Get data loaders\n",
        "train_loader, valid_loader = get_data(processed_sentences, processed_sentences_test)\n"
      ],
      "metadata": {
        "id": "pTCKGBgsKuxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Word2VecParams:\n",
        "\n",
        "    # skipgram parameters\n",
        "    MIN_FREQ = 50\n",
        "    SKIPGRAM_N_WORDS = 8\n",
        "    T = 85\n",
        "    NEG_SAMPLES = 50\n",
        "    NS_ARRAY_LEN = 5_000_000\n",
        "    SPECIALS = \"\"\n",
        "    TOKENIZER = 'basic_english'\n",
        "\n",
        "    # network parameters\n",
        "    BATCH_SIZE = 100\n",
        "    EMBED_DIM = 300\n",
        "    EMBED_MAX_NORM = None\n",
        "    N_EPOCHS = 5\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    CRITERION = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "Lj60MYqCK8As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Word2VecParams:\n",
        "\n",
        "    # skipgram parameters\n",
        "    MIN_FREQ = 50\n",
        "    SKIPGRAM_N_WORDS = 8\n",
        "    T = 85\n",
        "    NEG_SAMPLES = 50\n",
        "    NS_ARRAY_LEN = 5_000_000\n",
        "    SPECIALS = \"\"\n",
        "    TOKENIZER = 'basic_english'\n",
        "\n",
        "    # network parameters\n",
        "    BATCH_SIZE = 100\n",
        "    EMBED_DIM = 300\n",
        "    EMBED_MAX_NORM = None\n",
        "    N_EPOCHS = 5\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    CRITERION = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "irb4QZ0cVQzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, list, specials):\n",
        "        self.stoi = {v[0]:(k, v[1]) for k, v in enumerate(list)}\n",
        "        self.itos = {k:(v[0], v[1]) for k, v in enumerate(list)}\n",
        "        self._specials = specials[0]\n",
        "        self.total_tokens = np.nansum(\n",
        "            [f for _, (_, f) in self.stoi.items()]\n",
        "            , dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi) - 1\n",
        "\n",
        "    def get_index(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[0]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[0]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[0])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[0])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def get_freq(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[1]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[1]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[1])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[1])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def lookup_token(self, token: Union[int, List]):\n",
        "        if isinstance(token, (int, np.int64)):\n",
        "            if token in self.itos:\n",
        "                return self.itos.get(token)[0]\n",
        "            else:\n",
        "                raise ValueError(f\"Token {token} not in vocabulary\")\n",
        "        elif isinstance(token, list):\n",
        "            res = []\n",
        "            for t in token:\n",
        "                if t in self.itos:\n",
        "                    res.append(self.itos.get(token)[0])\n",
        "                else:\n",
        "                    raise ValueError(f\"Token {t} is not a valid index.\")\n",
        "            return res\n"
      ],
      "metadata": {
        "id": "pgzLn-hLVUvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(iterator, tokenizer):\n",
        "    r = re.compile('[a-z1-9]')\n",
        "    for text in iterator:\n",
        "        text = ' '.join(text) if isinstance(text, list) else text\n",
        "        res = tokenizer(text)\n",
        "        res = list(filter(r.match, res))\n",
        "        yield res\n",
        "\n",
        "def vocab(ordered_dict: Dict, min_freq: int = 1, specials: str = ''):\n",
        "    tokens = []\n",
        "    # Save room for special tokens\n",
        "    for token, freq in ordered_dict.items():\n",
        "        if freq >= min_freq:\n",
        "            tokens.append((token, freq))\n",
        "\n",
        "    specials = (specials, np.nan)\n",
        "    tokens[0] = specials\n",
        "\n",
        "    return Vocab(tokens, specials)\n",
        "\n",
        "def pipeline(word, vocab, tokenizer):\n",
        "    return vocab(tokenizer(word))\n",
        "\n",
        "def build_vocab(\n",
        "        iterator,\n",
        "        tokenizer,\n",
        "        params: Word2VecParams,\n",
        "        max_tokens: Optional[int] = None,\n",
        "    ):\n",
        "    counter = Counter()\n",
        "    for tokens in yield_tokens(iterator, tokenizer):\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # First sort by descending frequency, then lexicographically\n",
        "    sorted_by_freq_tuples = sorted(\n",
        "        counter.items(), key=lambda x: (-x[1], x[0])\n",
        "        )\n",
        "\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "    word_vocab = vocab(\n",
        "        ordered_dict, min_freq=params.MIN_FREQ, specials=params.SPECIALS\n",
        "        )\n",
        "    return word_vocab\n"
      ],
      "metadata": {
        "id": "aKqnrs3OVZU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGrams:\n",
        "    def __init__(self, vocab: Vocab, params: Word2VecParams, tokenizer):\n",
        "        self.vocab = vocab\n",
        "        self.params = params\n",
        "        self.t = self._t()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.discard_probs = self._create_discard_dict()\n",
        "\n",
        "    def _t(self):\n",
        "        freq_list = []\n",
        "        for _, (_, freq) in list(self.vocab.stoi.items())[1:]:\n",
        "            freq_list.append(freq/self.vocab.total_tokens)\n",
        "        return np.percentile(freq_list, self.params.T)\n",
        "\n",
        "\n",
        "    def _create_discard_dict(self):\n",
        "        discard_dict = {}\n",
        "        for _, (word, freq) in self.vocab.stoi.items():\n",
        "            dicard_prob = 1-np.sqrt(\n",
        "                self.t / (freq/self.vocab.total_tokens + self.t))\n",
        "            discard_dict[word] = dicard_prob\n",
        "        return discard_dict\n",
        "\n",
        "\n",
        "    def collate_skipgram(self, batch):\n",
        "        batch_input, batch_output  = [], []\n",
        "        for text in batch:\n",
        "            text_tokens = self.vocab.get_index(self.tokenizer(text))\n",
        "\n",
        "            if len(text_tokens) < self.params.SKIPGRAM_N_WORDS * 2 + 1:\n",
        "                continue\n",
        "\n",
        "            for idx in range(len(text_tokens) - self.params.SKIPGRAM_N_WORDS*2\n",
        "                ):\n",
        "                token_id_sequence = text_tokens[\n",
        "                    idx : (idx + self.params.SKIPGRAM_N_WORDS * 2 + 1)\n",
        "                    ]\n",
        "                input_ = token_id_sequence.pop(self.params.SKIPGRAM_N_WORDS)\n",
        "                outputs = token_id_sequence\n",
        "\n",
        "                prb = random.random()\n",
        "                del_pair = self.discard_probs.get(input_)\n",
        "                if input_==0 or del_pair >= prb:\n",
        "                    continue\n",
        "                else:\n",
        "                    for output in outputs:\n",
        "                        prb = random.random()\n",
        "                        del_pair = self.discard_probs.get(output)\n",
        "                        if output==0 or del_pair >= prb:\n",
        "                            continue\n",
        "                        else:\n",
        "                            batch_input.append(input_)\n",
        "                            batch_output.append(output)\n",
        "\n",
        "        batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "        batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "        return batch_input, batch_output"
      ],
      "metadata": {
        "id": "JGkrWoAHVc0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSampler:\n",
        "    def __init__(self, vocab: Vocab, ns_exponent: float, ns_array_len: int):\n",
        "        self.vocab = vocab\n",
        "        self.ns_exponent = ns_exponent\n",
        "        self.ns_array_len = ns_array_len\n",
        "        self.ns_array = self._create_negative_sampling()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ns_array)\n",
        "\n",
        "    def _create_negative_sampling(self):\n",
        "\n",
        "        frequency_dict = {word:freq**(self.ns_exponent) \\\n",
        "                          for _,(word, freq) in\n",
        "                          list(self.vocab.stoi.items())[1:]}\n",
        "        frequency_dict_scaled = {\n",
        "            word:\n",
        "            max(1,int((freq/self.vocab.total_tokens)*self.ns_array_len))\n",
        "            for word, freq in frequency_dict.items()\n",
        "            }\n",
        "        ns_array = []\n",
        "        for word, freq in tqdm(frequency_dict_scaled.items()):\n",
        "            ns_array = ns_array + [word]*freq\n",
        "        return ns_array\n",
        "\n",
        "    def sample(self,n_batches: int=1, n_samples: int=1):\n",
        "        samples = []\n",
        "        for _ in range(n_batches):\n",
        "            samples.append(random.sample(self.ns_array, n_samples))\n",
        "        samples = torch.as_tensor(np.array(samples))\n",
        "        return samples"
      ],
      "metadata": {
        "id": "2WQVuHQ-Vfnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab: Vocab, params: Word2VecParams):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.t_embeddings = nn.Embedding(\n",
        "            self.vocab.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "        self.c_embeddings = nn.Embedding(\n",
        "            self.vocab.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs, context):\n",
        "        # getting embeddings for target & reshaping\n",
        "        target_embeddings = self.t_embeddings(inputs)\n",
        "        n_examples = target_embeddings.shape[0]\n",
        "        n_dimensions = target_embeddings.shape[1]\n",
        "        target_embeddings = target_embeddings.view(n_examples, 1, n_dimensions)\n",
        "\n",
        "        # get embeddings for context labels & reshaping\n",
        "        # Allows us to do a bunch of matrix multiplications\n",
        "        context_embeddings = self.c_embeddings(context)\n",
        "        # * This transposes each batch\n",
        "        context_embeddings = context_embeddings.permute(0,2,1)\n",
        "\n",
        "        # * custom linear layer\n",
        "        dots = target_embeddings.bmm(context_embeddings)\n",
        "        dots = dots.view(dots.shape[0], dots.shape[2])\n",
        "        return dots\n",
        "\n",
        "    def normalize_embeddings(self):\n",
        "        embeddings = list(self.t_embeddings.parameters())[0]\n",
        "        embeddings = embeddings.cpu().detach().numpy()\n",
        "        norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "        norms = norms.reshape(norms.shape[0], 1)\n",
        "        return embeddings / norms\n",
        "\n",
        "    def get_similar_words(self, word, n):\n",
        "        word_id = self.vocab.get_index(word)\n",
        "        if word_id == 0:\n",
        "            print(\"Out of vocabulary word\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word_vec = embedding_norms[word_id]\n",
        "        word_vec = np.reshape(word_vec, (word_vec.shape[0], 1))\n",
        "        dists = np.matmul(embedding_norms, word_vec).flatten()\n",
        "        topN_ids = np.argsort(-dists)[1 : n + 1]\n",
        "\n",
        "        topN_dict = {}\n",
        "        for sim_word_id in topN_ids:\n",
        "            sim_word = self.vocab.lookup_token(sim_word_id)\n",
        "            topN_dict[sim_word] = dists[sim_word_id]\n",
        "        return topN_dict\n",
        "\n",
        "    def get_similarity(self, word1, word2):\n",
        "        idx1 = self.vocab.get_index(word1)\n",
        "        idx2 = self.vocab.get_index(word2)\n",
        "        if idx1 == 0 or idx2 == 0:\n",
        "            print(\"One or both words are out of vocabulary\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word1_vec, word2_vec = embedding_norms[idx1], embedding_norms[idx2]\n",
        "\n",
        "        return cosine(word1_vec, word2_vec)"
      ],
      "metadata": {
        "id": "QJPme-18VgrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model: Model, params: Word2VecParams, optimizer,\n",
        "                vocab: Vocab, train_iter, valid_iter, skipgrams: SkipGrams):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.vocab = vocab\n",
        "        self.train_iter = train_iter\n",
        "        self.valid_iter = valid_iter\n",
        "        self.skipgrams = skipgrams\n",
        "        self.params = params\n",
        "\n",
        "        self.epoch_train_mins = {}\n",
        "        self.loss = {\"train\": [], \"valid\": []}\n",
        "\n",
        "        # sending all to device\n",
        "        self.model.to(self.params.DEVICE)\n",
        "        self.params.CRITERION.to(self.params.DEVICE)\n",
        "\n",
        "        self.negative_sampler = NegativeSampler(\n",
        "            vocab=self.vocab, ns_exponent=.75,\n",
        "            ns_array_len=self.params.NS_ARRAY_LEN\n",
        "            )\n",
        "        self.testwords = ['learning', 'computer', 'machine', 'vision', 'language', 'network', 'technology', 'science', 'internet']\n",
        "\n",
        "\n",
        "    # def train(self):\n",
        "    #     self.test_testwords()\n",
        "    #     for epoch in range(self.params.N_EPOCHS):\n",
        "    #         # Generate Dataloaders\n",
        "    #         self.train_dataloader = DataLoader(\n",
        "    #             self.train_iter,\n",
        "    #             batch_size=self.params.BATCH_SIZE,\n",
        "    #             shuffle=False,\n",
        "    #             collate_fn=self.skipgrams.collate_skipgram\n",
        "    #         )\n",
        "    #         self.valid_dataloader = DataLoader(\n",
        "    #             self.valid_iter,\n",
        "    #             batch_size=self.params.BATCH_SIZE,\n",
        "    #             shuffle=False,\n",
        "    #             collate_fn=self.skipgrams.collate_skipgram\n",
        "    #         )\n",
        "    #         # training the model\n",
        "    #         st_time = monotonic()\n",
        "    #         self._train_epoch()\n",
        "    #         self.epoch_train_mins[epoch] = round((monotonic()-st_time)/60, 1)\n",
        "\n",
        "    #         # validating the model\n",
        "    #         self._validate_epoch()\n",
        "    #         print(f\"\"\"Epoch: {epoch+1}/{self.params.N_EPOCHS}\\n\"\"\",\n",
        "    #         f\"\"\"    Train Loss: {self.loss['train'][-1]:.2}\\n\"\"\",\n",
        "    #         f\"\"\"    Valid Loss: {self.loss['valid'][-1]:.2}\\n\"\"\",\n",
        "    #         f\"\"\"    Training Time (mins): {self.epoch_train_mins.get(epoch)}\"\"\"\n",
        "    #         \"\"\"\\n\"\"\"\n",
        "    #         )\n",
        "    #         self.test_testwords()\n",
        "\n",
        "    def train(self):\n",
        "        try:\n",
        "            self.test_testwords()\n",
        "            for epoch in range(self.params.N_EPOCHS):\n",
        "                # Directly use self.train_iter and self.valid_iter\n",
        "                self.train_dataloader = self.train_iter\n",
        "                self.valid_dataloader = self.valid_iter\n",
        "\n",
        "                # training the model\n",
        "                st_time = monotonic()\n",
        "                self._train_epoch()\n",
        "                self.epoch_train_mins[epoch] = round((monotonic()-st_time)/60, 1)\n",
        "\n",
        "                # validating the model\n",
        "                self._validate_epoch()\n",
        "                print(f\"\"\"Epoch: {epoch+1}/{self.params.N_EPOCHS}\\n\"\"\",\n",
        "                f\"\"\"    Train Loss: {self.loss['train'][-1]:.2}\\n\"\"\",\n",
        "                f\"\"\"    Valid Loss: {self.loss['valid'][-1]:.2}\\n\"\"\",\n",
        "                f\"\"\"    Training Time (mins): {self.epoch_train_mins.get(epoch)}\"\"\"\n",
        "                \"\"\"\\n\"\"\"\n",
        "                )\n",
        "                self.test_testwords()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Done Execution\")\n",
        "\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
        "            if len(batch_data[0]) == 0:\n",
        "                continue\n",
        "            inputs = batch_data[0].to(self.params.DEVICE)\n",
        "            pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "            neg_labels = self.negative_sampler.sample(\n",
        "                pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                )\n",
        "            neg_labels = neg_labels.to(self.params.DEVICE)\n",
        "            context = torch.cat(\n",
        "                [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                neg_labels], dim=1\n",
        "              )\n",
        "\n",
        "            # building the targets tensor\n",
        "            y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "            y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "            y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs, context)\n",
        "            loss = self.params.CRITERION(outputs, y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "\n",
        "        self.loss['train'].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(self.valid_dataloader, 1):\n",
        "                if len(batch_data[0]) == 0:\n",
        "                    continue\n",
        "                inputs = batch_data[0].to(self.params.DEVICE)\n",
        "                pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "                neg_labels = self.negative_sampler.sample(\n",
        "                    pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                    ).to(self.params.DEVICE)\n",
        "                context = torch.cat(\n",
        "                    [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                    neg_labels], dim=1\n",
        "                  )\n",
        "\n",
        "\n",
        "                # building the targets tensor\n",
        "                y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "                y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "                y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "                preds = self.model(inputs, context).to(self.params.DEVICE)\n",
        "                loss = self.params.CRITERION(preds, y)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "            epoch_loss = np.mean(running_loss)\n",
        "            self.loss['valid'].append(epoch_loss)\n",
        "\n",
        "    def test_testwords(self, n: int = 5):\n",
        "        for word in self.testwords:\n",
        "            print(word)\n",
        "            nn_words = self.model.get_similar_words(word, n)\n",
        "            for w, sim in nn_words.items():\n",
        "                print(f\"{w} ({sim:.3})\", end=' ')\n",
        "            print('\\n')"
      ],
      "metadata": {
        "id": "BwaNx98uVmsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = Word2VecParams()\n",
        "train_iter, valid_iter = get_data(processed_sentences, processed_sentences_test)\n",
        "tokenizer = get_tokenizer(params.TOKENIZER)\n",
        "vocab = build_vocab(train_iter, tokenizer, params)\n",
        "skip_gram = SkipGrams(vocab=vocab, params=params, tokenizer=tokenizer)\n",
        "model = Model(vocab=vocab, params=params).to(params.DEVICE)\n",
        "optimizer = torch.optim.Adam(params = model.parameters())"
      ],
      "metadata": {
        "id": "nQDCJyB0Vn9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_iter:\n",
        "    print(batch)\n",
        "    break  # Only print the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOZwy7SJXp10",
        "outputId": "c6112a8e-bac0-4ade-bd1d-1373989cfd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['combination spatial separable convolution mainly used enable amispp perform convolution regions different aspect ratios', 'information contains kind value', 'modern scenarios often deal massive high dimensional complex data spawned multiple sources natural consider ad perspective multiview deep learning', 'based byzantine fault tolerant bft mechanism paper improved anomaly detection method sdn multi controller fault tolerant abnormal wrong instructions ensured flow table issued correctly', 'using electro spinning method pvdf based nanofiber coated coper electrodes', 'mobile young people five million people world possessing great abilities', 'obtained mouse behavior data consist outliers behavioral variability addressed peirce criterion weighted least square regression wlsr respectively', 'sdn architectures decouples network control forwarding functions making network control become directly programmable underlying infrastructure abstracted applications network services', 'chapter two networking terror information age editors abstract', 'conventionally usability testing often conducted many individual participants', 'paper proposes scalable reconfigurable ml platform power plant based docker technologies support online model deployment execution scheduling', 'hand interpretation images mainly depends image characteristics observed naked eye inevitably affected subjective factors personal experience', 'considering necessity automation present distress data collection increasingly shifted towards atomization', 'methods successful locating optimal solution usually slow convergence require much computing time', 'single scoring model scorecard built entire customer population', 'present algorithm generate best possible query plan answer sparql protocol rdf query language sparql query based cost model', 'methodology finds application several real scenarios like trajectory tracking parking docking agvs', 'order hri closed dialog researchers need advertise hri greater robotics community sub discipline full new interesting application domains', 'mouse operating behavior users captured coordinate axes elapsed time based movement mouse', 'addition results outline relationship ict womens capabilities social relations education knowledge paid work projects bodily integrity safety potentially promising research', 'broad goal democratize ai says co author song han assistant professor electrical engineering computer science researcher microsystems technology laboratories mit', 'could observe allowing user explore open urban data using specific visualizations may lead effective data interpretation', 'significant differences found five successive measurements one person three teats', 'partial inner product spaces theory applications partial inner product pip spaces ubiquitous eg', 'hoped issue serve catalytic agent future research similar areas', 'data fusion processing algorithms consider position sensors using quaternion representation', 'following training sites data transformed latent data aggregated training', 'rather questions flexible researcher requires may evolve new data becomes available ideas grow andor epiphanies emerge', 'several investigations hospitals demonstrated presence staphylococcus aureus bacillus spp enterococcus spp streptococcus spp escherichia coli proteus spp klebsiella spp pseudomonas aeruginosa acinetobacter coliforms mobile phones used medical staff four hundred sixtyseven students seven patients eight point', 'training completed making use three thousand six hundred fiftythree photographs eleven different objects visually impaired individuals may encounter', 'review cyber physical systems based blockchain possibilities challenges cyber physical systems cps internet things iot booming technologies researchers industries closely correlated perform task oriented operations', 'furthermore three cnn svm showed best performance binary classification']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        params=params,\n",
        "        optimizer=optimizer,\n",
        "        train_iter=train_iter,\n",
        "        valid_iter=valid_iter,\n",
        "        vocab=vocab,\n",
        "        skipgrams=skip_gram\n",
        "    )\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ZvrlLiWY9B",
        "outputId": "360d430b-20cd-442a-df74-e6e55b4873f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1514/1514 [00:08<00:00, 176.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning\n",
            "price (0.196) primary (0.196) account (0.181) might (0.158) surface (0.157) \n",
            "\n",
            "computer\n",
            "classical (0.164) visualization (0.155) position (0.155) optical (0.153) mathematical (0.151) \n",
            "\n",
            "machine\n",
            "analytics (0.203) features (0.2) likely (0.171) difference (0.153) caused (0.149) \n",
            "\n",
            "vision\n",
            "tool (0.177) failure (0.171) day (0.167) dynamic (0.164) independent (0.158) \n",
            "\n",
            "language\n",
            "comprehensive (0.212) range (0.199) essential (0.185) introducing (0.183) face (0.169) \n",
            "\n",
            "network\n",
            "fuzzy (0.205) internet (0.182) public (0.166) share (0.166) center (0.165) \n",
            "\n",
            "technology\n",
            "quantum (0.194) well (0.177) objective (0.175) services (0.174) guide (0.172) \n",
            "\n",
            "science\n",
            "needs (0.188) made (0.187) twelve (0.174) day (0.156) expert (0.154) \n",
            "\n",
            "internet\n",
            "either (0.187) network (0.182) opportunities (0.168) price (0.166) features (0.166) \n",
            "\n",
            "Done Execution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Above the embeddings for the most similar words that shows the the testing of this model."
      ],
      "metadata": {
        "id": "D3joDELwIVhe"
      }
    }
  ]
}